<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>welcome</title>
    <link rel="stylesheet" href="font.css">
</head>
<body>
    <p>This article is about the machine learning technique. For other kinds of random tree, see Random tree.
        Part of a series on
        Machine learning
        and data mining
        Paradigms
        Problems
        Supervised learning
        (classification • regression)
        Apprenticeship learningDecision treesEnsembles BaggingBoostingRandom forestk-NNLinear regressionNaive BayesArtificial neural networksLogistic regressionPerceptronRelevance vector machine (RVM)Support vector machine (SVM)
        Clustering
        Dimensionality reduction
        Structured prediction
        Anomaly detection
        Artificial neural network
        Reinforcement learning
        Learning with humans
        Model diagnostics
        Mathematical foundations
        Machine-learning venues
        Related articles
        vte
        Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned.[1][2] Random decision forests correct for decision trees' habit of overfitting to their training set.[3]: 587–588 
        
        The first algorithm for random decision forests was created in 1995 by Tin Kam Ho[1] using the random subspace method,[2] which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg.[4][5][6]
        
        An extension of the algorithm was developed by Leo Breiman[7] and Adele Cutler,[8] who registered[9] "Random Forests" as a trademark in 2006 (as of 2019, owned by Minitab, Inc.).[10] The extension combines Breiman's "bagging" idea and random selection of features, introduced first by Ho[1] and later independently by Amit and Geman[11] in order to construct a collection of decision trees with controlled variance.
    </p>
</body>
</html>